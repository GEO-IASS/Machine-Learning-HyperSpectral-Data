# -*- coding: utf-8 -*-
"""
Created on Fri Jul 11 09:31:40 2014

@author: SpectralMD2
"""

print(__doc__)

import numpy as np
import pylab as pl

from sklearn import clone
from sklearn.datasets import load_iris
from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,
                              AdaBoostClassifier)
from sklearn.externals.six.moves import xrange
from sklearn.tree import DecisionTreeClassifier


# Paramters

n_classes = 3
n_estimators = 30
plot_colors = "ryb"
cmap = pl.cm.Accent
plot_step = 0.02 # fine step width for decision for decison surface contours
plot_step_coarser = 0.5
RANDOM_SEED = 13

iris = load_iris()

plot_idx = 1

#models = [DecisionTreeClassifier(max_depth=None),
#          RandomForestClassifier(n_estimators = n_estimators),
#        ExtraTreesClassifier(n_estimators(max_depth=3),n_estimators =n_estimators)]
        

models = [DecisionTreeClassifier(max_depth=None),
          RandomForestClassifier(n_estimators=n_estimators),
          ExtraTreesClassifier(n_estimators=n_estimators),
          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),
                             n_estimators=n_estimators)]










for pair in ([0,1],[0,2],[2,3]):
    for model in models:
        X = iris.data[:,pair]
        y = iris.target
        
        # shuffle
        idx = np.arange(X.shape[0])
        np.random.seed(RANDOM_SEED)
        np.random.shuffle(idx)
        X = X[idx]
        y = y[idx]
        
        # standardize
        mean = X.mean(axis=0)
        std = X.std(axis=0)
        X = (X-mean)/std
        
        #Train
        
        clf = clone(model)
        clf = model.fit(X,y)
        
        scores = clf.score(X,y)
        
        # create a title for each column and the console by using str() and 
        # slciing away uselss parts of the string
        
        model_title = str(type(model)).split(".")[-1][:-2][:-len("Classifier")]
        
        model_details = model_title
        
        if hasattr(model, "estimators_"):
#            model_detail +="with {} estimators".format(len(model.esfeature_importances_))
            model_details += " with {} estimators".format(len(model.estimators_))
        print model_details + " with features", pair, "has a score of", scores
        print "                 "
#        pl.subplot(3,4,plot_idx)
        pl.subplot(3, 4, plot_idx)
        if plot_idx <=len(models):
            pl.title(model_title)
            
        # Create a title for each colunm and the console by using str() and
        # slicing away uselesss parts of the string
        x_min, x_max = X[:,0].min()-1, X[:,0].max()+1
        y_min,y_max = X[:,1].min()-1, X[:,1].max() +1
        
        xx, yy = np.meshgrid(np.arange(x_min,x_max,plot_step),
                             np.arange(y_min,y_max,plot_step))
                             
       # plot either a single decisiontree classifier or alpha blend the decison
                             # sufufaces of the ensemble of classifiers
                             
                             
                             
        if isinstance(model,DecisionTreeClassifier):
            Z = model.predict(np.c_[xx.ravel(),yy.ravel()])
            Z = Z.reshape(xx.shape)
            cs= pl.contourf(xx,yy,Z,cmap = cmap)
        
        else:
            # Choose alpha blend level with respect to the number of estimators
        # that are in use 9nothing that adaboost can use fewer estimators than its maximum if it achieves a good though
        # a good enoug fit early on
            estimator_alpha = 1.0 / len(model.estimators_)
#            for tree in model.efeature_importances_:
            for tree in model.estimators_:
                Z = tree.predict(np.c_[xx.ravel(),yy.ravel()])
                Z = Z.reshape(xx.shape)
                cs = pl.contourf(xx,yy,Z,alpha = estimator_alpha,cmap = cmap)
            
        # build a coarse trid to plot a set of ensmbel classification
        # to show how these are different to what we see in the decision
        # surfaces, these points are regularly space and do not have a black outline
        
        xx_coarser,yy_coarser = np.meshgrid(np.arange(x_min,x_max,plot_step_coarser),
                                            np.arange(y_min,y_max,plot_step_coarser))
                                            
#        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),yy_coarser()]).reshape(xx_coarser.shape)
        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(), yy_coarser.ravel()]).reshape(xx_coarser.shape)
        cs_points = pl.scatter(xx_coarser,yy_coarser,s=15, c =Z_points_coarser,cmap=cmap,edgecolors="none")
        
        for i, c in zip(xrange(n_classes),plot_colors):
            idx = np.where(y==i)
            pl.scatter(X[idx,0],X[idx,1],c=c,label=iris.target_names[i],cmap=cmap)
        plot_idx +=1
pl.suptitle("Classifiers on feature subsets of the Iris dataset")
pl.axis("tight")
pl.show()
     
        
            
                                         
                             
                             
                             
                             
                  
